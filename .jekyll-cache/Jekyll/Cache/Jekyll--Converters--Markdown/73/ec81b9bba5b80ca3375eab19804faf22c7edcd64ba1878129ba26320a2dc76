I"ëA<h1 id="æ·±åº¦åº¦é‡å­¦ä¹ ">æ·±åº¦åº¦é‡å­¦ä¹ </h1>

<h2 id="è·ç¦»åº¦é‡å­¦ä¹ ">è·ç¦»åº¦é‡å­¦ä¹ </h2>
<p>åœ¨æœç´¢ä»»åŠ¡ä¸­ï¼Œç»™å®šæŸ¥è¯¢æ ·æœ¬å’Œå€™é€‰é›†åˆï¼Œæˆ‘ä»¬ä¸€èˆ¬é‡‡ç”¨çš„æ­¥éª¤æ˜¯ï¼š1ï¼‰æå–æ ·æœ¬ç‰¹å¾ï¼›2ï¼‰è®¡ç®—æŸ¥è¯¢ä¸å€™é€‰æ ·æœ¬ç‰¹å¾ä¹‹é—´çš„è·ç¦»ï¼›3ï¼‰è¿”å›è·ç¦»æœ€å°çš„å€™é€‰ä½œä¸ºæœç´¢ç»“æœã€‚</p>

<p>å¸¸ç”¨çš„åº¦é‡æ ·æœ¬ä¹‹é—´è·ç¦»çš„æ–¹æ³•åŒ…æ‹¬æ¬§å¼è·ç¦»ï¼Œä½™å¼¦è·ç¦»ï¼Œæ±‰æ˜è·ç¦»ç­‰ã€‚ç„¶è€Œå•ä¸€çš„è·ç¦»åº¦é‡æ–¹å¼éš¾ä»¥é€‚ç”¨ä¸åŒåœºæ™¯ä¸‹çš„æœç´¢ä»»åŠ¡ï¼Œå·²æœ‰çš„è·ç¦»æ–¹å¼æœ¬èº«ä¹Ÿå¯èƒ½å­˜åœ¨ç¼ºé™·ï¼Œå¦‚æ¬§å¼è·ç¦»å‡è®¾ç‰¹å¾æ‰€æœ‰ç»´åº¦çš„æƒé‡ç›¸åŒï¼Œå› æ­¤å¦‚ä½•ä»æ•°æ®ä¸­å­¦ä¹ å‡ºæœ‰æ•ˆçš„è·ç¦»åº¦é‡æˆä¸ºè®¸å¤šç ”ç©¶è€…å…³æ³¨çš„é—®é¢˜ã€‚</p>

<p><strong>è·ç¦»åº¦é‡å­¦ä¹ ï¼ˆDistance Metric Learningï¼‰</strong>ç®—æ³•ä¸€èˆ¬æ˜¯å­¦ä¹ ä¸€ä¸ªé©¬æ°çŸ©é˜µï¼Œä»è€Œä¸¤ä¸ªæ ·æœ¬ç‚¹ <script type="math/tex">\boldsymbol{x}_{i}</script> å’Œ $\boldsymbol{x}_{j}$ ä¹‹é—´çš„è·ç¦»å®šä¹‰ä¸º</p>

<p>â€‹                                                     <script type="math/tex">D_{\boldsymbol{M}}(\boldsymbol{x}_{i}, \boldsymbol{x}_{j}) = (\boldsymbol{x}_{i}-\boldsymbol{x}_{j})^\top \boldsymbol{M} (\boldsymbol{x}_{i}-\boldsymbol{x}_{j})</script></p>

<p>è·ç¦»åº¦é‡å­¦ä¹ åœ¨äººè„¸éªŒè¯å’Œè¡Œäººå†è¯†åˆ«åœºæ™¯ä¸­ç ”ç©¶è¾ƒå¤šï¼Œå¦‚ <a href="http://jmlr.csail.mit.edu/papers/volume10/weinberger09a/weinberger09a.pdf">Margin Nearest Neighbor Learning (LMNN)</a>ï¼Œ<a href="http://www.cs.utexas.edu/users/pjain/pubs/metriclearning_icml.pdf">Information Theoretic Metric Learning (ITML)</a> ï¼Œ<a href="https://data.vision.ee.ethz.ch/cvl/mguillau/publications/Guillaumin2009iccv2_poster.pdf">Logistic Discriminant Metric Learning (LDML)</a> ï¼Œ<a href="https://www.tugraz.at/fileadmin/user_upload/Institute/ICG/Documents/lrs/pubs/koestinger_cvpr_2012.pdf">KISSME</a>ï¼Œ <a href="https://zpascal.net/cvpr2015/Liao_Person_Re-Identification_by_2015_CVPR_paper.pdf">XQDA</a>ï¼Œ<a href="https://www.eecs.qmul.ac.uk/~sgg/papers/ZhengGongXiang_CVPR2011.pdf">Probabilistic Relative Distance Comparison (PRDC)</a> ç­‰ã€‚</p>

<p>åŸºäºè·ç¦»åº¦é‡çŸ©é˜µå­¦ä¹ çš„æ–¹æ³•è™½ç„¶å¤šç§å¤šæ ·ï¼Œæœ¬è´¨å‡æ˜¯åŸºäº<strong>åŒ¹é…æ ·æœ¬è·ç¦»å°äºéåŒ¹é…æ ·æœ¬è·ç¦»</strong>çš„å‡è®¾æ¥å®šä¹‰ä¸åŒçš„ç›®æ ‡å‡½æ•°å’Œçº¦æŸæ¡ä»¶ï¼Œä¸”æ±‚è§£æ–¹æ³•å¤šç§å¤šæ ·ã€‚è¯¥æ–¹æ³•åœ¨æœç´¢é˜¶æ®µä¸­ä¸€èˆ¬åˆ©ç”¨å­¦ä¹ åˆ°çš„é©¬æ°çŸ©é˜µå¿«é€Ÿè®¡ç®—ç‰¹å¾é—´çš„è·ç¦»ï¼Œæ•ˆç‡è¾ƒé«˜ã€‚</p>

<h2 id="æ·±åº¦åº¦é‡å­¦ä¹ -1">æ·±åº¦åº¦é‡å­¦ä¹ </h2>
<p>éšç€æ·±åº¦å­¦ä¹ çš„å…´èµ·ï¼Œç ”ç©¶è€…ä»¬å¼€å§‹å…³æ³¨å¦‚ä½•åˆ©ç”¨æ·±åº¦ç¥ç»ç½‘ç»œå­¦ä¹ å¥½çš„æ ·æœ¬ç‰¹å¾ï¼Œä»è€Œåœ¨ç‰¹å¾æ˜ å°„ç©ºé—´ä¸­é‡‡ç”¨ç®€å•çš„æ¬§å¼æˆ–ä½™å¼¦è·ç¦»å³å¯æ­£ç¡®åº¦é‡æ ·æœ¬ä¹‹é—´çš„è·ç¦»ï¼Œæˆ‘ä»¬å°†å…¶ç§°ä¸º<strong>æ·±åº¦åº¦é‡å­¦ä¹ ï¼ˆDeep Metric Learningï¼‰</strong>ã€‚</p>

<h3 id="ç»å…¸æŸå¤±å‡½æ•°">ç»å…¸æŸå¤±å‡½æ•°</h3>

<p>æ·±åº¦åº¦é‡å­¦ä¹ çš„ç ”ç©¶é‡ç‚¹åœ¨äºå¦‚ä½•å®šä¹‰ä¸€ä¸ªå¥½çš„æŸå¤±å‡½æ•°ï¼Œæ¥æŒ‡å¯¼ç½‘ç»œå­¦ä¹ åˆ°å…·æœ‰åˆ¤åˆ«èƒ½åŠ›çš„ç‰¹å¾ã€‚ç»å…¸çš„åº¦é‡å­¦ä¹ æŸå¤±å‡½æ•°åŒ…æ‹¬å¯¹æ¯”æŸå¤±å‡½æ•°ï¼ˆContrastive Lossï¼‰å’Œä¸‰å…ƒç»„æŸå¤±å‡½æ•°ï¼ˆTriplet Lossï¼‰ã€‚</p>

<h4 id="contrastive-loss">Contrastive Loss</h4>

<p>å¯¹æ¯”æŸå¤±å‡½æ•°ï¼ˆ<a href="http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf">Contrastive Loss</a>ï¼‰èµ·æºäº Yann LeCun ç­‰äººåœ¨CVPR 2006ä¸Šæå‡ºçš„ Dimensionality Reduction by Learning an Invariant Mapping (DrLIM) ç®—æ³•ï¼Œå…¶ä¸»è¦æ€æƒ³æ˜¯åœ¨å­¦ä¹ é™ç»´æ˜ å°„å‡½æ•°æ—¶ï¼Œç›¸ä¼¼çš„è¾“å…¥è¦è¢«æ˜ å°„åˆ°ä½ç»´ç©ºé—´ç›¸è¿‘çš„ç‚¹ï¼Œè€Œä¸ç›¸ä¼¼çš„è¾“å…¥åˆ™è¦è¢«æ˜ å°„åˆ°ç›¸è·è¾ƒè¿œçš„ç‚¹ã€‚</p>

<p>ç»™å®šä¸¤ä¸ªè¾“å…¥æ ·æœ¬ <script type="math/tex">X_{i}</script> å’Œ <script type="math/tex">X_{j}</script> ï¼Œä»¥åŠåŒ¹é…æ ‡ç­¾ <script type="math/tex">y_{i,j} \in \{0, 1\}</script>ï¼Œå…¶ä¸­ <script type="math/tex">y_{i,j}=1</script> è¡¨ç¤ºä¸¤ä¸ªæ ·æœ¬ç›¸ä¼¼ï¼Œ<script type="math/tex">y_{i,j}=0</script> è¡¨ç¤ºä¸ç›¸ä¼¼ï¼Œé™ç»´æ˜ å°„å‡½æ•°<script type="math/tex">G_{\boldsymbol{W}}</script>ï¼Œå¯¹æ¯”æŸå¤±å‡½æ•°å®šä¹‰ä¸º
â€‹<center>$$L_{contrast}=y_{i,j}  \frac{1}{2} D_{i,j}^2 +  (1 - y_{i,j}) \frac{1}{2} [m - D_{i,j}^2]_+$$</center>
å…¶ä¸­ <script type="math/tex">D_{i,j}=\|G_{\boldsymbol{W}}(X_{i}) - G_{\boldsymbol{W}}(X_{j})\|_{2}</script> ä¸ºæ˜ å°„å‘é‡ä¹‹é—´çš„æ¬§å¼è·ç¦»ï¼Œhingeå‡½æ•° $[z]_+=\max(z,0)$ï¼Œ$m$ ä¸ºé˜ˆå€¼å‚æ•°ã€‚è¯¥å…¬å¼è¡¨æ˜æˆ‘ä»¬å¸Œæœ›<strong>ç›¸ä¼¼æ ·æœ¬ä¹‹é—´çš„è·ç¦»å°½å¯èƒ½å°ï¼Œè€Œä¸ç›¸ä¼¼æ ·æœ¬ä¹‹é—´çš„è·ç¦»è¦å¤§äºé˜ˆå€¼m</strong>ã€‚</p>

<h4 id="triplet-loss">Triplet Loss</h4>

<p>ä¸‰å…ƒç»„æŸå¤±å‡½æ•°ï¼ˆ<a href="https://arxiv.org/pdf/1503.03832.pdf">Triplet Loss</a>ï¼‰ç”± Florian Schroff ç­‰äººåœ¨ CVPR 2016ä¸Šæå‡ºï¼Œå…¶ä¸»è¦æ€æƒ³æ˜¯æœ€å°åŒ–ç›®æ ‡æ ·æœ¬ä¸æ­£æ ·æœ¬ä¹‹é—´çš„è·ç¦»ï¼ŒåŒæ—¶æœ€å¤§åŒ–ç›®æ ‡æ ·æœ¬ä¸è´Ÿæ ·æœ¬ä¹‹é—´çš„è·ç¦»ã€‚</p>

<p><img src="/assets/metriclearning/triplet.png" alt="triplet" /></p>

<p>ç»™å®šç›®æ ‡æ ·æœ¬ <script type="math/tex">\boldsymbol{x}_{i}^{a}</script>  ï¼Œæ­£æ ·æœ¬ <script type="math/tex">\boldsymbol{x}_{i}^{p}</script> å’Œè´Ÿæ ·æœ¬ <script type="math/tex">\boldsymbol{x}_{i}^{n}</script> ï¼Œä»¥åŠç½‘ç»œ <script type="math/tex">f</script>ï¼Œä¸‰å…ƒç»„æŸå¤±å‡½æ•°å®šä¹‰ä¸º
â€‹<center>$$L_{tri}= [\|f(\boldsymbol{x}_{i}^{a}) - f(\boldsymbol{x}_{i}^{p})\|_{2}^{2} - \|f(\boldsymbol{x}_{i}^{a}) - f(\boldsymbol{x}_{i}^{n})\|_2^{2} + m, 0]_{+} $$</center>
å…¶ä¸­ $m$ ä¸ºé˜ˆå€¼å‚æ•°ï¼Œä½œè€…åœ¨è®¡ç®—æ ·æœ¬é—´æ¬§å¼è·ç¦»å‰å¯¹ç‰¹å¾è¿›è¡Œäº†å½’ä¸€åŒ–ï¼Œå³ <script type="math/tex">\|f(\boldsymbol{x})\|_{2}=1</script>ã€‚è¯¥å…¬å¼è¡¨æ˜æˆ‘ä»¬å¸Œæœ›æ­£æ ·æœ¬å¯¹ä¸è´Ÿæ ·æœ¬å¯¹ä¹‹é—´çš„è·ç¦»é—´éš”è‡³å°‘ä¸º$m$ ã€‚</p>

<p>ä»å®é™…ç»éªŒæ¥çœ‹ï¼ŒTriplet Loss åœ¨å¾ˆå¤šæƒ…å†µä¸‹è¡¨ç°ä¼šæ¯” Contrastive Loss å¥½ä¸€äº›ï¼Œç›®å‰ä¹Ÿå·²å¹¿æ³›åº”ç”¨äºå„ç§æ¶‰åŠåˆ°åº¦é‡å­¦ä¹ çš„ä»»åŠ¡ä¸­ã€‚Quora ä¸Šè®¨è®ºäº†ä¸€äº›å¯èƒ½çš„åŸå› ï¼š</p>

<blockquote>
  <ol>
    <li>
      <p>Triplet Loss is less â€œgreedyâ€ï¼ŒContrastive Loss è¦æ±‚ç›¸ä¼¼æ ·æœ¬é—´çš„è·ç¦»è¶Šå°è¶Šå¥½ï¼Œä½† Triplet Loss ä»…è¦æ±‚æ ·æœ¬å¯¹è·ç¦»ä¹‹é—´çš„é—´éš”æ»¡è¶³é˜ˆå€¼èŒƒå›´ï¼›</p>
    </li>
    <li>
      <p>è·ç¦»æœ¬è´¨ä¸Šæ˜¯ä¸ªç›¸å¯¹æ¦‚å¿µï¼Œå½“æˆ‘ä»¬è®¤ä¸º <script type="math/tex">\boldsymbol{x}_{i}^{a}</script> ä¸ <script type="math/tex">\boldsymbol{x}_{i}^{p}</script> ä¹‹é—´ç›¸è·è¾ƒè¿‘æ—¶ï¼Œæ˜¯å› ä¸ºç›¸æ¯”ä¹‹ä¸‹ $\boldsymbol{x}<em>{i}^{a}<script type="math/tex">ä¸</script>\boldsymbol{x}</em>{i}^{n}<script type="math/tex">ä¹‹é—´ç›¸è·æ›´è¿œï¼›è€Œå½“æˆ‘ä»¬æŠŠ  $\boldsymbol{x}_{i}^{n}</script> ç§»åŠ¨å¾—ä¸ $\boldsymbol{x}<em>{i}^{a}<script type="math/tex">æ›´è¿‘æ—¶ï¼Œé‚£æˆ‘ä»¬å°±ä¸å†è®¤ä¸º</script>\boldsymbol{x}</em>{i}^{a}<script type="math/tex">ä¸ $\boldsymbol{x}_{i}^{p}</script> ç›¸è·å¾ˆè¿‘äº†ï¼›Triplet Loss æ˜¯ä»ç›¸å¯¹è·ç¦»ä¸Šè¿›è¡Œçº¦æŸï¼Œè€Œ Contrastive Loss åªè€ƒè™‘ä¸¤ä¸ªæ ·æœ¬ç‚¹ä¹‹é—´çš„ç»å¯¹è·ç¦»ã€‚</p>
    </li>
  </ol>

  <p>â€” <a href="https://www.quora.com">What are the advantage of using triplet loss function over a contrastive loss? How would you decide which to use?</a></p>
</blockquote>

<p>ç¬”è€…ä¸ªäººæ›´å€¾å‘äºç¬¬ä¸€ç§è§£é‡Šï¼Œå°½ç®¡æˆ‘ä»¬å¸Œæœ›ç›¸ä¼¼æ ·æœ¬è·ç¦»è¿‘ï¼Œä¸ç›¸ä¼¼æ ·æœ¬è·ç¦»è¿œï¼Œä½† Contrastive Loss å¹¶ä¸å¯¹ç›¸ä¼¼æ ·æœ¬çš„è·ç¦»è¿›è¡Œé™åˆ¶ï¼Œç½‘ç»œåœ¨å­¦ä¹ è¿‡ç¨‹ä¸­ä¼šä¸æ–­æœ€å°åŒ–ç›¸ä¼¼æ ·æœ¬è·ç¦»ï¼Œè¿™æ ·å®¹æ˜“å¯¼è‡´è¿‡æ‹Ÿåˆï¼Œæˆ–æ‰å…¥è¾ƒå·®çš„æå°ç‚¹ã€‚å°½ç®¡è·ç¦»è¿œè¿‘æ˜¯ç›¸å¯¹çš„ï¼Œä½†æˆ‘ä»¬è®¡ç®—çš„ï¼ˆå½’ä¸€åŒ–ï¼‰æ¬§å¼æˆ–ä½™å¼¦è·ç¦»ä¼šæœ‰ä¸ªèŒƒå›´ï¼ˆå¦‚ï¼»0ï¼Œ2ï¼½ä¹‹é—´ï¼‰ï¼Œç»å¯¹è·ç¦»è¿˜æ˜¯å¯ä»¥æä¾›è¿œè¿‘ä¿¡æ¯ï¼Œå¦‚æˆ‘ä»¬è¿˜æ˜¯è®¤ä¸º1.3è·ç¦»è¾ƒè¿œï¼Œ0.1è·ç¦»è¾ƒè¿‘ã€‚ä¹Ÿæœ‰ç ”ç©¶è€…å°†ç»å¯¹è·ç¦»å’Œç›¸å¯¹è·ç¦»è¿›è¡Œç»¼åˆè€ƒè™‘æ¥è¿›ä¸€æ­¥æå‡ç®—æ³•æ€§èƒ½ã€‚</p>

<p>Triplet Loss è™½ç„¶åœ¨å¾ˆå¤šåº”ç”¨ä¸­è¡¨ç°å‡ºä¸é”™çš„æ€§èƒ½ï¼Œä½†å…¶ä¹Ÿå­˜åœ¨ç€ä¸¤å¤§é—®é¢˜ï¼š1ï¼‰å¦‚ä½•é€‰æ‹©ä¸‰å…ƒç»„ï¼Ÿ2ï¼‰å¦‚ä½•è®¾å®šé˜ˆå€¼ï¼Ÿ Schroff ç­‰äººæŒ‡å‡ºæˆ‘ä»¬å¹¶ä¸éœ€è¦ç¦»çº¿æšä¸¾æ‰€æœ‰çš„ä¸‰å…ƒç»„ï¼Œå› ä¸ºå¤§éƒ¨åˆ†ä¸‰å…ƒç»„æ»¡è¶³çº¦æŸæ¡ä»¶ï¼Œå¯¹è®­ç»ƒç½‘ç»œå¹¶æ²¡æœ‰è´¡çŒ®ï¼Œè¿˜ä¼šé™ä½æ”¶æ•›é€Ÿåº¦ã€‚ç„¶è€Œï¼Œé’ˆå¯¹æ¯ä¸ªæ ‡è®°çš„anchor-positive pairé€‰æ‹©æœ€éš¾è´Ÿæ ·æœ¬ï¼ˆhardest negativeï¼‰ä¹Ÿå¾ˆå®¹æ˜“å¯¼è‡´ç½‘ç»œæ‰å…¥è¾ƒå·®çš„æå°ç‚¹ï¼Œå› æ­¤ä»–ä»¬æå‡ºäº†ä¸€ç§ online semi-hard negative sampling  ç­–ç•¥ï¼Œå³åœ¨æ¯ä¸ªmini batchä¸­é€‰æ‹©åˆ°anchorè·ç¦»å¤§äºæ­£æ ·æœ¬ä½†å¹¶æ²¡æœ‰è¶…è¿‡ç›¸å¯¹è·ç¦»é˜ˆå€¼çš„è´Ÿæ ·æœ¬ã€‚ä¸ºäº†é€‰æ‹©æœ‰æ•ˆçš„ä¸‰å…ƒç»„ï¼Œä»–ä»¬åœ¨å®éªŒä¸­è®¾ç½®äº†è¾ƒå¤§çš„batch sizeï¼š</p>

<blockquote>
  <p>The main constraint with regards to the batch size, however, is the way we select hard relevant triplets from within the mini-batches. In most experiments we use a batch size of around 1,800 exemplars.</p>

  <p>â€” <a href="https://arxiv.org/pdf/1503.03832.pdf">FaceNet: A Unified Embedding for Face Recognition and Clustering</a></p>
</blockquote>

<p>é’ˆå¯¹é˜ˆå€¼é€‰æ‹©ï¼Œè¯¥æ–‡å¹¶æ²¡æœ‰è¿›è¡Œæ¯”è¾ƒæ·±å…¥çš„è®¨è®ºï¼Œä¸€èˆ¬æ ¹æ®ç»éªŒè®¾ç½® $m=0.2$ã€‚</p>

<p>ç ”ç©¶è€…ä»¬åœ¨ä¸åŒçš„åº”ç”¨åœºæ™¯ä¸­å¯¹ Triplet Loss è¿›è¡Œäº†ä¸€ç³»åˆ—æ”¹è¿›ï¼ŒChenç­‰äººåœ¨CVPR2017ä¸Šé’ˆå¯¹è¡Œäººå†è¯†åˆ«æå‡ºå››å…ƒç»„æŸå¤±å‡½æ•° <a href="http://zpascal.net/cvpr2017/Chen_Beyond_Triplet_Loss_CVPR_2017_paper.pdf">Quadruplet Loss</a>ï¼Œå®šä¹‰å¦‚ä¸‹ï¼š</p>
<center>$$L_{quad}= \sum \limits_{i,j,k} [g(\boldsymbol{x}_{i}, \boldsymbol{x}_{j})^2 - g(\boldsymbol{x}_{i}, \boldsymbol{x}_{k})^2 + \alpha_1]_{+} + \sum \limits_{i,j,k,l}[g(\boldsymbol{x}_{i}, \boldsymbol{x}_{j})^2 - g(\boldsymbol{x}_{l}, \boldsymbol{x}_{k})^2 + \alpha_1]_{+} $$
$$s.t. s_i = s_j, s_i \neq s_k, s_i \neq s_l, s_l \neq s_k$$</center>

<p>Quadruplet loss åœ¨ triplet lossï¼ˆç¬¬ä¸€é¡¹ï¼‰åŸºç¡€ä¸Šå¢åŠ äº†ä¸åŒç±»åˆ«è´Ÿæ ·æœ¬ä¹‹é—´çš„è·ç¦»çº¦æŸï¼ˆç¬¬äºŒé¡¹ï¼‰ï¼Œä»è€Œè¿›ä¸€æ­¥å¢å¤§å­¦ä¹ åˆ°ç‰¹å¾çš„ç±»é—´å·®å¼‚ã€‚</p>

<p>Wang ç­‰äººåœ¨ICCV2017ä¸Šæå‡º <a href="https://arxiv.org/pdf/1708.01682.pdf">Angular Loss</a>ï¼Œçº¦æŸä¸‰å…ƒç»„æ ·æœ¬æ„æˆä¸‰è§’å½¢ä¸­ä»¥$x_n$ä¸ºé¡¶ç‚¹çš„çš„å¤¹è§’è¶Šå°è¶Šå¥½ï¼Œå…¶å½¢å¼å¦‚ä¸‹ï¼š</p>
<center>$$L_{ang}=\left[\|\boldsymbol{x}_a - \boldsymbol{x}_p\|^2 - 4\tan^2\alpha\|\boldsymbol{x}_n-\boldsymbol{x}_c\|^2 \right]_+$$

å…¶ä¸­ $$\alpha$$ æ˜¯è§’åº¦è¶…å‚ï¼Œä¸€èˆ¬å– $$36^{\circ} \leq \alpha \leq 55^{\circ}$$ã€‚æœ‰æ„æ€çš„æ˜¯ï¼Œè¯¥æ–‡ä»æ±‚æ¢¯åº¦è§’åº¦é˜è¿° Angular Loss çš„ä¼˜è¶Šæ€§ï¼ŒTriplet Loss åœ¨æ±‚æ¢¯åº¦æ—¶ä»…ä¾èµ–ä¸¤ä¸ªæ ·æœ¬ç‚¹ï¼Œè€Œ Angular Loss åˆ™åŒæ—¶è€ƒè™‘äº†å‚ä¸æŸå¤±å‡½æ•°è®¡ç®—çš„ä¸‰ä¸ªæ ·æœ¬ç‚¹ã€‚

â€‹$$\frac{\partial L_{tri}}{\partial \boldsymbol{x}_n} = 2(\boldsymbol{x}_a -\boldsymbol{x}_n)~~~~~~~~~~~~~~~\frac{\partial L_{ang}}{\partial \boldsymbol{x}_n} = 4\tan^2\alpha[(\boldsymbol{x}_a +\boldsymbol{x}_p) - 2\boldsymbol{x}_n]$$

â€‹$$\frac{\partial L_{tri}}{\partial \boldsymbol{x}_p} = 2(\boldsymbol{x}_p -\boldsymbol{x}_a)~~~~~~~~~~~~~~~\frac{\partial L_{ang}}{\partial \boldsymbol{x}_p} = 2(\boldsymbol{x}_p -\boldsymbol{x}_a)-2\tan^2\alpha(\boldsymbol{x}_a +\boldsymbol{x}_p - 2\boldsymbol{x}_n)$$

â€‹$$\frac{\partial L_{tri}}{\partial \boldsymbol{x}_a} = 2(\boldsymbol{x}_n -\boldsymbol{x}_p)~~~~~~~~~~~~~~~\frac{\partial L_{ang}}{\partial \boldsymbol{x}_a} = 2(\boldsymbol{x}_a -\boldsymbol{x}_p)-2\tan^2\alpha(\boldsymbol{x}_a +\boldsymbol{x}_p - 2\boldsymbol{x}_n)$$



æ­¤å¤–ï¼Œè¿˜æœ‰å„ç§å„æ ·çš„ä¸‰å…ƒç»„æ ·æœ¬æŒ–æ˜ç­–ç•¥ï¼Œå¦‚ [Hard Sample Mining (HSM)](https://arxiv.org/pdf/1703.07737.pdf) å¯¹æ¯ä¸ªç›®æ ‡æ ·æœ¬é€‰æ‹©è·ç¦»æœ€å¤§çš„åŒç±»å›¾åƒå’Œè·ç¦»æœ€å°çš„ä¸åŒç±»å›¾åƒæ¥æ„å»ºä¸‰å…ƒç»„ï¼Œ[Margin Sample Mining Loss (MSML)](https://arxiv.org/pdf/1710.00478.pdf) ç»“åˆå››å…ƒç»„æŸå¤±å‡½æ•°å’Œéš¾æ ·æœ¬æŒ–æ˜ç­–ç•¥ï¼Œä»¥åŠ [Correcting the Triplet Selection Bias](http://openaccess.thecvf.com/content_ECCV_2018/papers/Baosheng_Yu_Correcting_the_Triplet_ECCV_2018_paper.pdf)ï¼Œ[Smart Mining](http://openaccess.thecvf.com/content_ICCV_2017/papers/Harwood_Smart_Mining_for_ICCV_2017_paper.pdf) å’Œ [Log-ratio Loss+Dense Triplet Mining](https://arxiv.org/pdf/1904.09626.pdf)ç­‰ã€‚è‡ªé€‚åº”é˜ˆå€¼é€‰å–ç­–ç•¥å¦‚ [Hierarchical Triplet Loss](http://openaccess.thecvf.com/content_ECCV_2018/papers/Ge_Deep_Metric_Learning_ECCV_2018_paper.pdf) æ„å»ºå±‚æ¬¡æ ‘åŠ¨æ€è®¡ç®—é˜ˆå€¼ï¼Œæ ·æœ¬ç”Ÿæˆç­–ç•¥å¦‚ [hardness-aware deep metric learning (HDML)](http://openaccess.thecvf.com/content_CVPR_2019/papers/Zheng_Hardness-Aware_Deep_Metric_Learning_CVPR_2019_paper.pdf) ç”±æ˜“åˆ°éš¾ç”Ÿæˆæ ·æœ¬æ¥ä¿è¯è®­ç»ƒè¿‡ç¨‹çš„é«˜æ•ˆæ€§ï¼ˆè¿™ä¸ [Curriculum Learning](https://ronan.collobert.com/pub/matos/2009_curriculum_icml.pdf) çš„æ€è·¯æœ‰ç‚¹åƒï¼Œç½‘ç»œå…ˆå­¦ä¹ ç®€å•çš„æ ·æœ¬ï¼Œå†æ…¢æ…¢å­¦ä¹ æ›´éš¾çš„æ ·æœ¬ï¼Œä»è€Œå¸®åŠ©ç½‘ç»œåŠ é€Ÿæ”¶æ•›å¹¶æ‰¾åˆ°æ›´å¥½çš„å±€éƒ¨æœ€ä¼˜ï¼‰ã€‚

### åŸºäºæ‰¹é‡æ•°æ®è®¡ç®—çš„æŸå¤±å‡½æ•°

è€ƒè™‘åˆ°ä¸‰å…ƒç»„æŸå¤±å‡½æ•°ä»…ä»ä¸€ä¸ªæ‰¹é‡ä¸­é€‰æ‹©æ­£è´Ÿæ ·æœ¬å¯¹ï¼Œå¹¶ä¸èƒ½å……åˆ†åˆ©ç”¨æ‰¹é‡ä¸­æ‰€æœ‰æ ·æœ¬çš„ä¿¡æ¯ï¼Œä¸€äº›ç ”ç©¶è€…æå‡ºåŸºäºæ‰¹é‡æ•°æ®è®¡ç®—çš„æŸå¤±å‡½æ•°ï¼ŒåŒ…æ‹¬Liftedstructï¼ŒHistogram Lossï¼ŒN-pair Lossç­‰ã€‚

#### Liftedstruct

[Liftedstruct](https://arxiv.org/pdf/1511.06452.pdf) ç”± Song ç­‰äººåœ¨ CVPR2016 ä¸Šæå‡ºï¼Œå®šä¹‰å¦‚ä¸‹

â€‹$$J = \frac{1}{2\hat{\mathcal{P}}} \sum \limits_{(i,j) \in \hat{\mathcal{P}}} \max(0, J_{i,j})^2$$

â€‹$$J_{i,j} = D_{i,j} + \max \left( \max \limits_{(i,k) \in \hat{\mathcal{N}}} \alpha - D_{j,k}, \max \limits_{(j,l) \in \hat{\mathcal{N}}} \alpha - D_{j,l}\right) $$

ä»å…¬å¼ä¸Šçœ‹ï¼Œè¯¥æŸå¤±å‡½æ•°çš„åŸå§‹å½¢å¼ç±»ä¼¼ online triplet lossï¼Œåªä¸è¿‡æ˜¯é’ˆå¯¹æ¯å¯¹æ­£æ ·æœ¬ $$(\boldsymbol{x}_{i}, \boldsymbol{x}_{j})$$ é€‰æ‹©åŒæ–¹çš„hardest negative sample æ¥è®¡ç®—æŸå¤±ã€‚è€ƒè™‘åˆ°hingeå‡½æ•°çš„éå¹³æ»‘æ€§ï¼Œè¯¥æ–‡æå‡ºä¼˜åŒ–å…¶å¹³æ»‘ä¸Šç•Œ

â€‹$$\tilde{J}_{i,j} = D_{i,j} + \log \left( \sum \limits_{(i,k) \in \mathcal{N}} \exp(\alpha - D_{j,k}) ï¼‹ \sum \limits_{(j,l) \in \mathcal{N}} \exp(\alpha - D_{j,l})\right)$$

è¯¥å¹³æ»‘å½¢å¼çš„æŸå¤±å‡½æ•°åˆ™åˆ©ç”¨äº†æ¯ä¸ªæ ·æœ¬çš„æ‰€æœ‰è´Ÿæ ·æœ¬è¿›è¡ŒæŸå¤±è®¡ç®—ã€‚

#### Histogram Loss

[Histogram Loss](https://papers.nips.cc/paper/6464-learning-deep-embeddings-with-histogram-loss.pdf) ç”± Ustinova ç­‰äººåœ¨ NIPS 2016 ä¸Šæå‡ºï¼Œå…¶ä¸»è¦æ€æƒ³æ˜¯æœ€å°åŒ–æ­£æ ·æœ¬å¯¹å’Œè´Ÿæ ·æœ¬å¯¹çš„è·ç¦»åˆ†å¸ƒç›´æ–¹å›¾ä¹‹é—´çš„é‡å ï¼Œä½¿å¾—éšæœºé‡‡æ ·æ­£æ ·æœ¬å¯¹ç›¸ä¼¼æ€§å°äºè´Ÿæ ·æœ¬å¯¹ç›¸ä¼¼æ€§çš„æ¦‚ç‡è¶Šå°è¶Šå¥½ã€‚è¯¥æ€æƒ³æœ‰ç‚¹ç±»ä¼¼äºæˆ‘ä»¬å¸Œæœ›æ­£æ ·æœ¬ä¹‹é—´çš„è·ç¦»çš„æœ€å¤§å€¼å°½å¯èƒ½å°äºè´Ÿæ ·æœ¬ä¹‹é—´çš„è·ç¦»æœ€å°å€¼ã€‚

![Histogram](/assets/metriclearning/histogram.png)
 
å…·ä½“æ¥è¯´ï¼Œç»™å®šæ­£è´Ÿæ ·æœ¬é›†åˆ $$S^+$$ å’Œ $$S^-$$ï¼Œæˆ‘ä»¬å°† $$[-1, +1]$$ è·ç¦»åˆ’åˆ†ä¸º $$R$$ ä¸ªbin,æ¯ä¸ªbinçš„å®½åº¦ä¸º $$\Delta =\frac{2}{R-1}$$ï¼ŒèŠ‚ç‚¹ä¸º $$t_1=-1, t_2, ...,t_R=+1$$ï¼Œè·ç¦»åˆ†å¸ƒç›´æ–¹å›¾ $$H^+$$ çš„æ¯ä¸ª bin ä¸Šçš„å€¼ $$h_r^+$$ è®¡ç®—å¦‚ä¸‹ï¼š

â€‹$$h_r^+ = \frac{1}{S^+} \sum \limits_{m_{i,j}=+1} \delta_{i,j,r}$$ï¼Œ

â€‹$$\begin{split} \delta_{i,j,r} = ~~~~&amp;(s_{i,j} - t_{r-1})/\Delta~~~~~~~~~if~~s_{i,j} \in [t_{r-1}; t_r]; \\
~~~~~~&amp;(t_{r+1} - s_{i,j})/\Delta~~~~~~~~~if~~s_{i,j} \in [t_{r}; t_{r+1}]; \\
~~~~~~&amp;~~~~~~~~~~~0~~~~~~~~~~~~~~~~~~ otherwise; \end{split}$$

Histogram Loss è®¡ç®—ä¸º $$L_{hist}=\sum \limits_{r=1}^R h_r^- \left(\sum \limits_{q=1}^r h_q^+ \right)=\sum \limits_{r=1}^R h_r^- \phi_r^+$$ï¼Œå…¶ä¸­ $$\phi_r^+$$ ä¸ºç›´æ–¹å›¾ $$H^+$$ çš„ç´¯ç§¯å’Œã€‚

Histogram Loss é¿å…äº†Triplet Lossä¸­çš„é˜ˆå€¼å’Œæ ·æœ¬é€‰æ‹©ï¼Œä»…éœ€è¦æ ¹æ®batchsizeè°ƒèŠ‚binçš„æ•°é‡ï¼Œåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå–å¾—äº†è¾ƒå¥½çš„ç»“æœã€‚æ­¤å¤–ï¼ŒHistogram Loss éœ€è¦è¾ƒå¤§çš„batchsize (å¦‚256) æ¥æ›´å‡†ç¡®åœ°ä¼°è®¡è·ç¦»åˆ†å¸ƒæƒ…å†µã€‚

#### N-pair Loss

[N-pair Loss](http://www.nec-labs.com/uploads/images/Department-Images/MediaAnalytics/papers/nips16_npairmetriclearning.pdf) ç”± Kihyuk Sohn åœ¨ NIPS 2016 ä¸Šæå‡ºï¼Œå…¶æ€æƒ³æ˜¯é’ˆå¯¹æ¯ä¸ªç›®æ ‡æ ·æœ¬åŒºåˆ†å…¶æ­£æ ·æœ¬å’ŒNï¼1ä¸ªè´Ÿæ ·æœ¬ã€‚

![npair](/assets/metriclearning/npair.png)

N-pair Loss å®šä¹‰ä¸º

â€‹$$L_{npair}({x, x^+, \{x_i\}_{i=1}^{N-1}}; f) = \log(1+ \sum \limits_{i=1}^{N-1} \exp(f^{\top}f_i -f^{\top}f^+ )) = -\log \frac{\exp(f^{\top}f^+)}{\exp(f^{\top}f^+)+\sum_{i=1}^{N-1}\exp(f^{\top}f_i)}$$

è¯¥æŸå¤±å‡½æ•°ä¸å¤šåˆ†ç±» Softmax Loss ç±»ä¼¼ï¼Œå³åˆ¤æ–­ $$x$$ å±äº $$x^+$$ ç±»è€Œä¸å±äºå…¶ä»–è´Ÿæ ·æœ¬ç±»ã€‚ä¸ Triplet Loss é‡‡ç”¨ä¸€ä¸ªè´Ÿæ ·æœ¬ç›¸æ¯”ï¼ŒN-pair Loss é‡‡ç”¨ $$N-1$$ ä¸ªè´Ÿæ ·æœ¬å¯ä»¥åŠ é€Ÿæ¨¡å‹æ”¶æ•›ã€‚è¯¥æ–‡ä¹Ÿæå‡ºäº†ä¸€ç§ hard negative class mining ç­–ç•¥ï¼Œä»å¤§é‡ä¸åŒç±»åˆ«ä¸­é€‰å–Cç±»ï¼Œéšæœºé€‰å–1ç±»åå†é€æ¬¡é€‰æ‹©è¿èƒŒè·ç¦»çº¦æŸçš„å…¶ä»–ç±»åˆ«ã€‚

## æ€»ç»“

æ·±åº¦åº¦é‡å­¦ä¹ çš„ç›®æ ‡æ˜¯å­¦ä¹ å¥½çš„**ç‰¹å¾**ï¼Œç ”ç©¶çš„é‡ç‚¹åœ¨äºå¦‚ä½•è®¾è®¡æŸå¤±å‡½æ•°ï¼Œä¸‰å…ƒç»„æŸå¤±å‡½æ•°ç›®å‰åº”ç”¨è¾ƒå¹¿ï¼Œä½†ä»ä¾èµ–äºç²¾å·§çš„å®ç°ï¼ŒåŒ…æ‹¬è®¡ç®—è·ç¦»æ—¶æ˜¯å¦è¦å¯¹ç‰¹å¾å½’ä¸€åŒ–ï¼Œé‡‡ç”¨æ¬§å¼è·ç¦»è¿˜æ˜¯ä½™å¼¦è·ç¦»ï¼Œé’ˆå¯¹ä¸ç”¨ç±»å‹çš„æ•°æ®é›†å¦‚ä½•é€‰æ‹©æ­£è´Ÿæ ·æœ¬å¯¹ï¼Œå¦‚ä½•é€‰å–é˜ˆå€¼ç­‰ã€‚åŸºäºæ‰¹é‡æ•°æ®è®¡ç®—çš„åº¦é‡å­¦ä¹ èƒ½å¤Ÿé¿å…ä¸‰å…ƒç»„æŸå¤±å‡½æ•°ä¸­ç¹ççš„æ ·æœ¬å’Œé˜ˆå€¼é€‰æ‹©é—®é¢˜ï¼Œä½†ä»éœ€è¦æ›´å¤šå®é™…ä»»åŠ¡æ¥éªŒè¯å…¶æœ‰æ•ˆæ€§ã€‚
</center>
:ET