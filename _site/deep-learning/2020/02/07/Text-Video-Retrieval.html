<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>文本视频搜索 Text-to-Video Retrieval</title>
  <meta name="description" content="文本视频搜索 Text-to-Video Retrieval">
  
  <meta name="author" content="Ying Zhang">
  <meta name="copyright" content="&copy; Ying Zhang 2020">
  

  <!-- External libraries -->
  <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css">
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.9.0/styles/monokai-sublime.min.css">
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/lightbox2/2.7.1/css/lightbox.css">

  <!-- Favicon and other icons (made with http://www.favicon-generator.org/) -->
  <link rel="shortcut icon" href="/assets/icons/write.png" type="image/x-icon">
  <link rel="icon" href="/assets/icons/write.png" type="image/x-icon">
  <link rel="apple-touch-icon" sizes="57x57" href="/assets/icons/write.png">
  <link rel="apple-touch-icon" sizes="60x60" href="/assets/icons/write.png">
  <link rel="apple-touch-icon" sizes="72x72" href="/assets/icons/write.png">
  <link rel="apple-touch-icon" sizes="76x76" href="/assets/icons/write.png">
  <link rel="apple-touch-icon" sizes="114x114" href="/assets/icons/write.png">
  <link rel="apple-touch-icon" sizes="120x120" href="/assets/icons/write.png">
  <link rel="apple-touch-icon" sizes="144x144" href="/assets/icons/write.png">
  <link rel="apple-touch-icon" sizes="152x152" href="/assets/icons/write.png">
  <link rel="apple-touch-icon" sizes="180x180" href="/assets/icons/write.png">
  <link rel="icon" type="image/png" sizes="192x192"  href="/assets/icons/write.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/assets/icons/write.png">
  <link rel="icon" type="image/png" sizes="96x96" href="/assets/icons/write.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/assets/icons/write.png">
  <link rel="manifest" href="/assets/icons/manifest.json">
  <meta name="msapplication-TileColor" content="#ffffff">
  <meta name="msapplication-TileImage" content="/assets/icons/write.png">
  <meta name="theme-color" content="#ffffff">

  
  <!-- Facebook OGP cards -->
  <meta property="og:description" content="文本视频搜索 Text-to-Video Retrieval" />
  <meta property="og:url" content="http://localhost:4000/deep-learning/2020/02/07/Text-Video-Retrieval.html">
  <meta property="og:site_name" content="Ying Zhang" />
  <meta property="og:title" content="文本视频搜索 Text-to-Video Retrieval" />
  <meta property="og:type" content="website" />
  <meta property="og:image" content="http://localhost:4000/assets/write.png" />
  <meta property="og:image:type" content="image/png" />
  <meta property="og:image:width" content="612" />
  <meta property="og:image:height" content="605" />
  

  
  <!-- Twitter: card tags -->
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="文本视频搜索 Text-to-Video Retrieval">
  <meta name="twitter:description" content="文本视频搜索 Text-to-Video Retrieval">
  <meta name="twitter:image" content="http://localhost:4000/assets/Y.png">
  <meta name="twitter:url" content="http://localhost:4000/deep-learning/2020/02/07/Text-Video-Retrieval.html">
  

  

  <!-- Site styles -->
  <link rel="stylesheet" href="/css/main.css">
  <link rel="canonical" href="http://localhost:4000/deep-learning/2020/02/07/Text-Video-Retrieval.html">
	<link rel="alternate" type="application/rss+xml" title="Ying Zhang" href="http://localhost:4000/feed.xml" />
	
	<!-- Tooltips -->
	<script type="text/javascript">
		window.tooltips = []
	</script>

  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
  <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
          tex2jax: {
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
          inlineMath: [['$','$']]
          }
      });
  </script>
</head>


  <body>

    <header class="navigation" role="banner">
  <div class="navigation-wrapper">
    <a href="/" class="logo">
      
      <img src="/assets/write.png" alt="Ying Zhang">
      
    </a>
    <a href="javascript:void(0)" class="navigation-menu-button" id="js-mobile-menu">
      <i class="fa fa-bars"></i>
    </a>
    <nav role="navigation">
      <ul id="js-navigation-menu" class="navigation-menu show">
				
	

	
	<li class="nav-link"><a href="/about/">About</a>
	

	

	

	
	<li class="nav-link"><a href="/posts/">Posts</a>
	

	
	<li class="nav-link"><a href="/typography/">Typography</a>
	

	

	

	

	

	

	

	


      </ul>
    </nav>
  </div>
</header>


	<div class="page-content">
        <div class="post">

<div class="post-header-container " >
  <div class="scrim ">
    <header class="post-header">
      <h1 class="title">文本视频搜索 Text-to-Video Retrieval</h1>
      <p class="info">by <strong>Ying Zhang</strong></p>
    </header>
  </div>
</div>

<div class="wrapper">

 <span class="page-divider">
  <span class="one"></span>
  <span class="two"></span>
</span>
 

<section class="post-meta">
  <div class="post-date">February 7, 2020</div>
  <div class="post-categories">
  in 
    
    <a href="/category/Deep-Learning">Deep-learning</a>
    
  
  </div>
</section>

<article class="post-content">
  <h1 id="文本视频搜索-text-to-video-retrieval">文本视频搜索 Text-to-Video Retrieval</h1>

<h2 id="前言">前言</h2>
<p>文本视频搜索（Text-to-Video Retrieval）是指给定一句文本描述，在视频库中查找相应视频。与图像文本匹配（Image-Text Matching）相似，研究者们致力于探索如何更好地度量文本和视频之间的相似性。然而相较于文本图像搜索，一方面视频数据采集标注和存储难度大，目前人工标记的高质量数据集较少；另一方面视频内容复杂多变、时长变化大、处理难度高，研究工作的进展也相对较慢。</p>

<p><img src="/assets/textvideoretrieval/text-to-video-retrieval.png" alt="text-to-video-retrieval" style="zoom:45%;" /></p>

<hr />

<h2 id="相关工作介绍">相关工作介绍</h2>

<p>基于深度学习的文本视频搜索研究主要围绕两个思路来进行，一是如何融合视频的多模态特征，如利用图像，音频，动作等信息来学习更强大的视频特征；二是如何更有效地编码视频和文本特征，如采用不同类型的特征编码网络来学习互补特征。</p>

<hr />

<h4 id="1-learning-joint-embedding-with-multimodal-cues-for-cross-modal-video-text-retrieval-icmr2018-pdf-code">[1] Learning Joint Embedding with Multimodal Cues for Cross-Modal Video-Text Retrieval, ICMR2018. [<a href="http://www.cs.cmu.edu/~fmetze/interACT/Publications_files/publications/ICMR2018_Camera_Ready.pdf">pdf</a>] [<a href="https://github.com/niluthpol/multimodal_vtt">code</a>]</h4>

<p><img src="/assets/textvideoretrieval/multimodal-cues.png" alt="multimodal-cues" style="zoom:60%;" /></p>

<p>该文基本思路是结合视频不同模态输入与文本的相似度来提升搜索性能。该文将视频的 Activity Feature (<a href="https://arxiv.org/pdf/1705.07750.pdf">RGB-I3D</a>) 和 Audio Feature (<a href="https://papers.nips.cc/paper/6146-soundnet-learning-sound-representations-from-unlabeled-video.pdf">SoundNet CNN</a>) 相融合来学习 Activity-Text 联合特征空间，并采用 Object Feature (<a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf">ResNet152</a>) 来学习 Object-Text 联合特征空间，其中文本特征采用 word embeddings 300 + GRU 进行学习。此外，采用加权排序损失函数 （weighted ranking loss）并选取最难负样本 (hardest negatives) 来提升算法性能。</p>

<hr />

<h4 id="2-learning-a-text-video-embedding-from-incomplete-and-heterogeneous-data-arxiv2020-pdf-code">[2] Learning a Text-Video Embedding from Incomplete and Heterogeneous Data, arXiv2020. [<a href="https://hal.archives-ouvertes.fr/hal-01975102/document">pdf</a>] [<a href="https://github.com/antoine77340/Mixture-of-Embedding-Experts">code</a>]</h4>

<p><img src="/assets/textvideoretrieval/incomplete-and-heterogeneous.png" alt="incomplete-and-heterogeneous" style="zoom: 50%;" /></p>

<p>该文基本思路是针对视频的每种输入特征学习单独的专家嵌入模型（expert embedding model），并由输入文本在线学习权重来融合多个专家模型预测出的相似度得分，从而提升搜索性能。每个专家嵌入模型由 Gated embedding module 实现，采用 self-gating 机制来重新校准不同维度的激活值，强化不同模型学习到特征嵌入的差异。该文强调了该算法可以处理不完整的输入视频特征，当缺少某种输入特征时，对可计算的权重进行重新归一化即可。该文用到的视频输入特征包括：</p>

<ul>
  <li>appearance features (<a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf">ResNet-152 pre-trained on ImageNet</a>)</li>
  <li>motion features (<a href="https://arxiv.org/pdf/1705.07750.pdf">I3D pre-trained on Kinetics</a>)</li>
  <li>audio features (<a href="">audio CNN</a>)</li>
  <li>face descriptor (<a href="http://dlib.net/">dlib for face detection and recognition</a>)。</li>
</ul>

<p>针对文本输入，采用 <a href="https://arxiv.org/pdf/1301.3781.pdf">word2vec trained on Google News</a> 提取单词特征， <a href="https://www.di.ens.fr/~josef/publications/Arandjelovic17.pdf">NetVLAD</a> 编码句子特征。</p>

<hr />

<h4 id="3-use-what-you-have-video-retrieval-using-representations-from-collaborative-experts-bmvc2019--pdf-code">[3] Use What You Have: Video Retrieval Using Representations From Collaborative Experts, BMVC2019.  [<a href="https://bmvc2019.org/wp-content/uploads/papers/0363-paper.pdf">pdf</a>] [<a href="https://github.com/albanie/collaborative-experts">code</a>]</h4>

<p><img src="/assets/textvideoretrieval/collaborative-experts.png" alt="collaborative-experts" style="zoom: 45%;" /></p>

<p>该文基本思路也是融合多种视频信息来学习更强大的视频表示。与上一篇类似，该文提出 Collaborative Gating，通过建模不同输入信息之间的关联来帮助学习多门限下的视频特征嵌入，并将多种特征嵌入相拼接作为最终的视频表示。该文采用的视频输入特征包括：</p>

<ul>
  <li>object features (<a href="https://arxiv.org/pdf/1709.01507.pdf">SENet-154 pretrained on ImageNet</a>)</li>
  <li>motion features (<a href="https://arxiv.org/pdf/1705.07750.pdf">I3D-inception</a>)</li>
  <li>face features (<a href="https://arxiv.org/pdf/1512.02325.pdf">SSD face detector</a> + <a href="https://www.robots.ox.ac.uk/~vgg/publications/2018/Cao18/cao18.pdf">ResNet50 pretrianed on VGGFace2</a>)</li>
  <li>audio features (<a href="https://arxiv.org/pdf/1609.09430.pdf">VGGish pretrained on YouTube-8m</a>)</li>
  <li>Speech-to-Text features (Google Cloud speech API + <a href="https://arxiv.org/pdf/1301.3781.pdf">word2vec trained on Google News</a>)</li>
  <li>OCR (<a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewFile/16469/16260">Pixel Link for text detection</a> + <a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Yang_Liu_Synthetically_Supervised_Feature_ECCV_2018_paper.pdf">CNN pretrianed on Synth90K</a> + <a href="https://arxiv.org/pdf/1301.3781.pdf">word2vec trained on Google News</a>)。</li>
</ul>

<p>针对文本，采用 <a href="https://arxiv.org/pdf/1301.3781.pdf">word2vec trained on Google News</a> 提取单词特征，<a href="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf">OpenAI-GPT</a> 来提取上下文关联的单词特征，<a href="https://www.di.ens.fr/~josef/publications/Arandjelovic17.pdf">NetVLAD</a> 编码句子特征。</p>

<hr />

<h4 id="4-dual-encoding-for-zero-example-video-retrieval-cvpr2019-pdf-code">[4] Dual Encoding for Zero-Example Video Retrieval, CVPR2019. [<a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Dong_Dual_Encoding_for_Zero-Example_Video_Retrieval_CVPR_2019_paper.pdf">pdf</a>] [<a href="https://github.com/danieljf24/dual_encoding">code</a>]</h4>

<p><img src="/assets/textvideoretrieval/dual-encoding.png" alt="dual-encoding" style="zoom: 45%;" /></p>

<p>该文主要思路是对视频输入和文本输入分别进行多层编码：基于Mean Pooling 的全局编码，基于 BiGRU 的时序编码，强化局部信息的 BiGRU-CNN 编码，将多层编码特征相拼接作为输出特征。该文采用 <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf">ResNet-152 pre-trained on ImageNet</a> 作为视频输入特征。针对文本，采用 <a href="https://arxiv.org/pdf/1709.01362.pdf">word2vec pretrained on Flickr English tags</a> 提取单词特征。</p>

<hr />

<h4 id="5-polysemous-visual-semantic-embedding-for-cross-modal-retrieval-cvpr2019-pdf-code">[5] Polysemous Visual-Semantic Embedding for Cross-Modal Retrieval, CVPR2019. [<a href="https://www.zpascal.net/cvpr2019/Song_Polysemous_Visual-Semantic_Embedding_for_Cross-Modal_Retrieval_CVPR_2019_paper.pdf">pdf</a>] [<a href="https://yalesong.github.io/pvse/">code</a>]</h4>

<p><img src="/assets/textvideoretrieval/pvse.png" alt="pvse" style="zoom: 50%;" /></p>

<p>该文基本思路是利用 multi-head self-attention+ residual module 学习 K 种 attention 加权的视频和文本特征，并采用 multiple-instance learning 来改进排序损失函数。该文提出 Diversity Loss 来强化不同权重融合后特征的差异性，以及 Domain Discrepancy Loss 来约束学习到的文本和视频特征分布相近。</p>

<p>针对视频输入，该文采用 <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf">ResNet-152 pre-trained on ImageNet</a> 提取图像特征， BiGRU 编码视频特征；针对文本输入，采用 <a href="https://nlp.stanford.edu/pubs/glove.pdf">GloVe pretrained on CommonCrawl</a> 提取单词特征，BiGRU 编码句子特征。</p>

<p>此外，该文提出了 <strong>MRW (my reaction when)</strong> 数据集，包括 50,107 个视频－文本数据对，视频内容为给定文本的动作或情绪反应。该数据集划分为 44,107 个训练片段，1,000 个验证片段，和 5,000 个测试片段。</p>

<hr />

<h4 id="6-a-joint-sequence-fusion-model-for-video-question-answering-and-retrieval-eccv2018-pdf-code">[6] A Joint Sequence Fusion Model for Video Question Answering and Retrieval, ECCV2018. [<a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Youngjae_Yu_A_Joint_Sequence_ECCV_2018_paper.pdf">pdf</a>] [<a href="https://github.com/yj-yu/lsmdc">code</a>]</h4>

<p><img src="/assets/textvideoretrieval/jsfusion.png" alt="jsfusion" style="zoom: 50%;" /></p>

<p>该文提出 JSFusion (Joint Sequence Fusion) 模型学习文本序列和视频序列的联合 3D 特征，CHD (Convolutional Hierarchical Decoder) 模型计算两个序列的匹配得分，可用于 retrieval 和 VQA 任务。针对视频输入，该文采用  <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf">ResNet-152 pre-trained on ImageNet</a> 提取图像特征，<a href="https://arxiv.org/pdf/1609.09430.pdf">VGGish</a> + PCA 提取音频特征；针对文本输入，该文采用 <a href="https://nlp.stanford.edu/pubs/glove.pdf">glove.42B.300d</a> 提取单词特征。</p>

<hr />

<h4 id="7-fine-grained-action-retrieval-through-multiple-parts-of-speech-embeddings-iccv2019-pdf-code">[7] Fine-Grained Action Retrieval Through Multiple Parts-of-Speech Embeddings, ICCV2019. [<a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Wray_Fine-Grained_Action_Retrieval_Through_Multiple_Parts-of-Speech_Embeddings_ICCV_2019_paper.pdf">pdf</a>] [<a href="https://mwray.github.io/FGAR/">code</a>]</h4>

<p><img src="/assets/textvideoretrieval/jpose.png" alt="jpose" style="zoom: 50%;" /></p>

<p>该文基本思路是利用 part-of-speech (PoS) parsing 对句子进行词性划分，针对每种词性（如名词，形容词，动此）学习相应的联合特征嵌入空间。针对视频输入，该文采用 <a href="https://wanglimin.github.io/papers/WangXWQLTV_ECCV16.pdf">TSN BNInception pretrained on Kinetics</a> 提取时空特征；针对文本输入，该文采用 Word2Vec pertained on Wikipedia corpus 来提取单词特征。</p>

<hr />

<h2 id="常用数据集">常用数据集</h2>

<ul>
  <li>
    <p><strong>MSR-VTT</strong>  由微软亚洲研究院在 <a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/06/cvpr16.msr-vtt.tmei_-1.pdf">CVPR2016</a> 上针对 video captioning 任务提出，该数据集包含 10K 个视频片段，总时长约 41.2h，每个片段约 20 个文本描述，共有 200K 个视频文本数据对。数据集划分为 6513 个训练视频，497 个验证视频，和 2990 个测试视频。</p>
  </li>
  <li>
    <p><strong>MSVD</strong> 在 <a href="http://www.cs.utexas.edu/~ml/papers/chen.acl11.pdf">NAACL-HLT 2011</a> 上提出，在 <a href="http://www.cs.utexas.edu/~ml/papers/chen.acl11.pdf">NAACL-HLT 2015</a> 上用于 video captioning，该数据集包含 1970 个视频片段，每个视频由一种或多种语言描述。该文对每个视频选取大约 40 个英文描述，并将数据集划分为1200 个训练视频，100 个验证视频，和 670 个测试视频。</p>
  </li>
  <li>
    <p><strong>LSMDC 2016</strong> 来自 <a href="https://sites.google.com/site/describingmovies/">Large Scale Movie Description Challenge</a>，相应论文发表在 <a href="https://link.springer.com/content/pdf/10.1007/s11263-016-0987-1.pdf">IJCV2017</a>。该数据集融合了 <strong>M-VAD</strong>（<a href="https://arxiv.org/pdf/1503.01070.pdf">Arxiv2015</a>）和 <strong>MPII-MD</strong>（<a href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Rohrbach_A_Dataset_for_2015_CVPR_paper.pdf">CVPR2015</a>）数据集，包含 200 部电影，总时长约147 h，共计 128,085个视频片段，和 128,118 个文本描述／台词。该数据集划分为 101,046 个训练片段和  7408 个验证片段。</p>
  </li>
  <li>其它数据集，包括 <a href="https://arxiv.org/pdf/1604.02748.pdf">TGIF</a>，<a href="https://www.zpascal.net/cvpr2019/Song_Polysemous_Visual-Semantic_Embedding_for_Cross-Modal_Retrieval_CVPR_2019_paper.pdf">MRW</a> 和 <a href="https://eccv2018.org/openaccess/content_ECCV_2018/papers/Dima_Damen_Scaling_Egocentric_Vision_ECCV_2018_paper.pdf">EPIC</a> 等。</li>
  <li>相关任务的数据集如 <a href="https://people.eecs.berkeley.edu/~lisa_anne/didemo.html">DiDeMo</a>，<a href="https://cs.stanford.edu/people/ranjaykrishna/densevid/">ActivityNet Captions</a>， <a href="https://www.aclweb.org/anthology/Q13-1003.pdf">TACoS</a>，<a href="https://github.com/jiyanggao/TALL">Charades-STA</a>，<a href="https://www.di.ens.fr/willow/research/howto100m/">HowTo100M</a>，<a href="https://vatex.org/main/index.html">VATEX</a> 等。</li>
</ul>

<p><img src="/assets/textvideoretrieval/howto100m.png" alt="howto100m" style="zoom: 40%;" /></p>

<hr />

<h2 id="总结">总结</h2>

<p>近年来文本视频搜索研究受到越来越多研究者的关注，实验结果表明，融合多种视频信息和多种网络编码结果可以帮助学习表达能力更强的联合特征，从而提升搜索性能。尽管有一些进展，其挑战依然存在：</p>

<ul>
  <li>如何保证实验结果的可对比性。从以上介绍的几篇论文可以看出，几乎每篇论文都采用了不同的视频输入特征或文本输入特征，难以保证算法对比的公平性。</li>
  <li>如何保证视频和文本的标记质量。目前的文本视频数据集存在问题：
    <ul>
      <li>“不完全适合”搜索任务：很多数据集是针对 video captioning 任务标记，一句文本描述会对应多个符合描述的视频，在搜索评测时会存在真值误差；</li>
      <li>文本质量欠佳：一些视频对应的文本描述可能是通过自动语音识别获得，并不能准确表达视频内容，且存在较大误差，影响学习到模型的准确性；</li>
      <li>视频质量欠佳：分辨率不一致，拍摄抖动，场景、目标、事件不连续，以及与内容无关的转场特效等。</li>
      <li>不完全对齐：一些标记文本仅对应部分视频内容，同一视频对应的文本标记相差较大。</li>
    </ul>
  </li>
  <li>如何更好地编码视频信息和文本信息。
    <ul>
      <li>目前常用的编码方式是 mean pooling，BiGRU， NetVLAD，或 multi-head attention。在同样输入情况下不同的编码方式并不能带来明显提升。如何更好的捕捉序列特征仍是值得进一步研究的问题。</li>
      <li>考虑到原始视频数据的处理难度较大，目前常用策略是预先提取视频的图像特征或动作特征，仅学习联合特征空间的映射函数。这在一定程度上会造成视频原始信息的损失，限制了算法性能。</li>
    </ul>
  </li>
  <li>如何更好地利用相关任务来帮助理解视频内容和文本信息，如视频分类，视频目标检测，视频动作检测，人脸检测与识别，场景分类等。</li>
</ul>

<p>除此之外，目前研究者们也致力于提出数据量更大、内容更广泛的数据集如 HowTo100M，来帮助文本视频表征学习，可以有效提升下游相关任务的性能。同时我们也看到，难的不是收集视频，而是获取准确的人工标记。因此，如何根据海量视频自带的信息（如标题／副标题／字幕／语音／背景音乐／人物等）来学习理解视频内容值得进一步探索。当然，这也是一项更复杂更庞大的工程。</p>

</article>



<section class="tags">
  <strong>Tags:</strong> <a href="/tag/deep-learning">deep-learning</a>,&nbsp;<a href="/tag/text-to-video-retrieval">text-to-video-retrieval</a>
</section>



<div id="gitmentContainer"></div>
<link rel="stylesheet" href="https://billts.site/extra_css/gitment.css">
<script src="https://billts.site/js/gitment.js"></script>
<script>
var gitment = new Gitment({
    owner: 'YingZhangDUT',
    repo: 'yingzhangdut.github.io',
    oauth: {
        client_id: 'Iv1.3fe7ed3cf3fd2140',
        client_secret: 'a28796df4b331a2f54248651288da6c9517cab9a',
    },
});
gitment.render('gitmentContainer');
</script>

<!-- <section class="rss">
  <p class="rss-subscribe text"><strong>Subscribe <a href="/feed.xml">via RSS</a></strong></p>
</section> -->

<!-- <section class="share">
  <span>Share: </span>
  
    
    
    
    
    
    
    
    
  
</section> -->





</div>
</div>

	</div>

    <footer class="site-footer">

  <div class="wrapper">

    <h3 class="footer-heading">Ying Zhang</h3>

    <div class="site-navigation">

      <p><strong>Site Map</strong></p>
      <ul class="pages">
				
	

	
	<li class="nav-link"><a href="/about/">About</a>
	

	

	

	
	<li class="nav-link"><a href="/posts/">Posts</a>
	

	
	<li class="nav-link"><a href="/typography/">Typography</a>
	

	

	

	

	

	

	

	


      </ul>
    </div>

    <div class="site-contact">

      <p><strong>Contact</strong></p>
      <ul class="social-media-list">
        <li>
          <a href="mailto:zydl0907@mail.dlut.edu.cn">
            <i class="fa fa-envelope-o"></i>
            <span class="username">zydl0907@mail.dlut.edu.cn</span>
          </a>
        </li>

        
          
          <li>
            <a href="https://github.com/YingZhangDUT" title="Fork me on GitHub">
              <i class="fa fa-github"></i>
              <span class="username">YingZhangDUT</span>
            </a>
          </li>
          
        

      </ul>
    </div>

    <!-- <div class="site-signature">
      <p class="rss-subscribe text"><strong>Subscribe <a href="/feed.xml">via RSS</a></strong></p>
      <p class="text">Keep Walking, Thinking, and Learning.
</p>
    </div> -->

  </div>

</footer>

<!-- Scripts -->
<script src="//code.jquery.com/jquery-3.4.1.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/highlight.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/lightbox2/2.11.1/js/lightbox.min.js"></script>
<script src="//unpkg.com/popper.js@1"></script>
<script src="//unpkg.com/tippy.js@5"></script>

<script type="text/javascript">
$(document).ready(function() {
  // Default syntax highlighting
  hljs.initHighlightingOnLoad();

  // Header
  var menuToggle = $('#js-mobile-menu').unbind();
  $('#js-navigation-menu').removeClass("show");
  menuToggle.on('click', function(e) {
    e.preventDefault();
    $('#js-navigation-menu').slideToggle(function(){
      if($('#js-navigation-menu').is(':hidden')) {
        $('#js-navigation-menu').removeAttr('style');
      }
    });
  });

	// Enable tooltips via Tippy.js
	if (Array.isArray(window.tooltips)) {
		window.tooltips.forEach(function(tooltip) {
			var selector = tooltip[0];
			var config = tooltip[1];
			tippy(selector, config);
		})
	}
});

</script>






  </body>

</html>
